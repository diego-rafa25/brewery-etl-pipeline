# Data Engineering ETL Project

This project implements a data engineering pipeline based on the Medallion architecture (Bronze â†’ Silver â†’ Gold), using Python, Pandas, Pytest, Docker, and Apache Airflow.  
It consumes public brewery data from the [Open Brewery DB](https://www.openbrewerydb.org), transforms it, and aggregates insights by state and brewery type.

---

## ğŸ“¦ Technologies Used

- Python 3.12  
- Pandas  
- Apache Airflow  
- Docker & Docker Compose  
- Pytest  
- Open Brewery DB API  

---

## ğŸ—‚ï¸ Pipeline Architecture

| Layer      | Description                                          |
|------------|------------------------------------------------------|
| **Bronze** | Raw data extracted from the API and saved as JSON    |
| **Silver** | Cleaned and transformed data saved as Parquet        |
| **Gold**   | Aggregated data grouped by state and brewery type    |

---

## ğŸš€ How to Run

### âœ”ï¸ Prerequisites

- Docker and Docker Compose installed
- A `.env` file in the project root with your credentials (see example below)

### â–¶ï¸ Run Locally (Without Airflow)

```bash
python run_local.py
```

---

## ğŸ§ª Unit Tests

Run unit tests for each ETL stage:

```bash
pytest tests/
```

---

## ğŸ“Š Test Coverage

To check coverage:

```bash
pytest --cov=etl tests/
```

To generate an HTML report:

```bash
pytest --cov=etl --cov-report=html tests/
start htmlcov/index.html
```

---

## ğŸ“§ Email Notification (Optional)

The DAG includes an optional task that sends an email after successful execution using Airflowâ€™s `EmailOperator`.

âš ï¸ Disabled by default to avoid SMTP configuration dependency.

### How to Enable

- Set up SMTP in `airflow.cfg` (host, port, username, password)  
- Uncomment `notify_success` in `brewery_dag.py`  
- Specify the recipient email in the `to` field  
- Chain the task: `task_aggregate >> notify_success`

ğŸ” **Important:** Never commit your credentials. Use environment variables or Airflow Connections.

---

## ğŸ³ Docker Setup

To start Airflow with Docker:

```bash
docker-compose up
```

Make sure the root folder includes a `.env` file:

```dotenv
POSTGRES_USER=airflow
POSTGRES_PASSWORD=your_secure_password
POSTGRES_DB=airflow
AIRFLOW_SECRET_KEY=your_random_secret_key
AIRFLOW_DB_URL=postgresql+psycopg2://airflow:your_secure_password@postgres/airflow
```

ğŸ“Œ Do not commit the `.env` file to the repository.

---

## ğŸ“ Project Structure

```
brewery-pipeline/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ aggregate.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â””â”€â”€ test_aggregate.py
â”œâ”€â”€ run_local.py
â”œâ”€â”€ validate.py
â”œâ”€â”€ brewery_dag.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env (not included)
â”œâ”€â”€ README.md
â””â”€â”€ README_pt-br.md
```

---

## ğŸ§¹ Cleaning Temporary Files

Remove auxiliary and cache files generated during local testing:

```bash
make clean
```

---

## ğŸ‘¨â€ğŸ’» Author

Project created by **Diego Rafael**.  
Feel free to connect on [LinkedIn](https://www.linkedin.com/in/diego-rafael-1057221a0/) or open an issue / pull request to collaborate!

---

## ğŸ“„ License

This project is intended for educational and technical exploration only.  
All data is provided publicly by Open Brewery DB.

Licensed under the [MIT License](LICENSE).